{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# fuel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import bz2\n",
    "import csv\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 980 (CNMeM is disabled)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/udi/Downloads/lisa'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fuel, os\n",
    "fuel_path = fuel.config.data_path[0]\n",
    "fuel_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base = 'data/r146_1_1/ivec15-lre/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_ivectors(filename):\n",
    "    \"\"\"Loads ivectors\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : string\n",
    "        Path to ivector files (e.g. dev_ivectors.csv)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    ids : list\n",
    "        List of ivectorids\n",
    "    durations : array, shaped('n_ivectors')\n",
    "        Array of durations for each ivectorid\n",
    "    languages : array, shaped('n_ivectors')\n",
    "        Array of langs for each ivectorid (only applies to train)\n",
    "    ivectors : array, shaped('n_ivectors', 600)\n",
    "        Array of ivectors for each ivectorid\n",
    "    \"\"\"\n",
    "    ids = []\n",
    "    durations = []\n",
    "    languages = []\n",
    "    ivectors = []\n",
    "    with open(filename, 'rb') as infile:\n",
    "        reader = csv.reader(infile, delimiter='\\t')\n",
    "        reader.next()\n",
    "\n",
    "        for row in csv.reader(infile, delimiter='\\t'):\n",
    "            ids.append(row[0])\n",
    "            durations.append(float(row[1]))\n",
    "            languages.append(row[2])\n",
    "            ivectors.append(np.asarray(row[3:], dtype=np.float32))\n",
    "\n",
    "            sys.stdout.write(\"\\r     %s  \" % row[0])\n",
    "            sys.stdout.flush()\n",
    "\n",
    "    print \"\\n   I-    Adding Transformed ivectors \"\n",
    "\n",
    "    return ids, np.array(durations, dtype=np.float32), np.array(languages), np.vstack(ivectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ivec15-lre_zzzzabb  \n",
      "   I-    Adding Transformed ivectors \n"
     ]
    }
   ],
   "source": [
    "train_ids, train_durations, train_languages, train_ivec = load_ivectors(base+'data/ivec15_lre_train_ivectors.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Nt = len(train_ivec)\n",
    "Nt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ivec15-lre_zzyykqa  \n",
      "   I-    Adding Transformed ivectors \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6431"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_ids, dev_durations, dev_languages, dev_ivec = load_ivectors(base+'data/ivec15_lre_dev_ivectors.tsv')\n",
    "len(dev_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ivec15-lre_zzshxfc  \n",
      "   I-    Adding Transformed ivectors \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6500"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ids, test_durations, test_languages, test_ivec = load_ivectors(base + 'data/ivec15_lre_test_ivectors.tsv')\n",
    "len(test_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute the mean and whitening transformation over dev set only. You are not allowed to use test and train does not have all languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = np.mean(dev_ivec, axis=0)\n",
    "S = np.cov(dev_ivec, rowvar=0)\n",
    "D, V = np.linalg.eig(S)\n",
    "W = (1 / np.sqrt(D) * V).transpose().astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "center and whiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_durations = np.hstack((train_durations,dev_durations,test_durations))\n",
    "all_data = np.vstack((train_ivec,dev_ivec,test_ivec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = np.dot(all_data - m, W.transpose())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert labels to int. 'out_of_set' is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: data/160111-fuel.idx2lang.pkl to s3://udikaggle/nist/160111-fuel.idx2lang.pkl\r\n"
     ]
    }
   ],
   "source": [
    "idx2lang = dict(enumerate(['out_of_set']+sorted(np.unique(train_languages))))\n",
    "lang2idx = dict((l,i) for i,l in idx2lang.iteritems())\n",
    "import cPickle as pickle\n",
    "with open('data/160111-fuel.idx2lang.pkl','wb') as fp:\n",
    "    pickle.dump(idx2lang,fp)\n",
    "!aws s3 cp data/160111-fuel.idx2lang.pkl s3://udikaggle/nist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = all_data\n",
    "y = np.array(map(lambda l: lang2idx[l], train_languages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mark all data not coming from training set as out of set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.hstack((y,lang2idx['out_of_set']*np.ones(len(X)-len(y),dtype=int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/udi/Downloads/lisa']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fuel\n",
    "fuel.config.data_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/udi/Downloads/lisa/160111-fuel.test/160111-fuel.test.hdf5'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from fuel.datasets.hdf5 import H5PYDataset\n",
    "datasource = '160111-fuel.test'\n",
    "datasource_dir = os.path.join(fuel.config.data_path[0], datasource)\n",
    "datasource_fname = os.path.join(datasource_dir , datasource + '.hdf5')\n",
    "datasource_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mkdir -p {datasource_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 udi  staff  44915848 Jan 11 17:30 /Users/udi/Downloads/lisa/160111-fuel.test/160111-fuel.test.hdf5\r\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "N, NF = X.shape\n",
    "with h5py.File(datasource_fname, mode='w') as fp:\n",
    "    features = fp.create_dataset('features', (N, NF), dtype=np.float32)\n",
    "    targets = fp.create_dataset('targets', (N,), dtype='int')\n",
    "    features[...] = X.astype(np.float32)\n",
    "    targets[...] = y\n",
    "    from fuel.datasets.hdf5 import H5PYDataset\n",
    "    split_dict = {\n",
    "        'train': {'features': (0, N), 'targets': (0, N)},\n",
    "        'test': {'features': (0, N), 'targets': (0, N)}\n",
    "    }\n",
    "    fp.attrs['split'] = H5PYDataset.create_split_array(split_dict)\n",
    "!ls -l {datasource_fname}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the samples are not shuffled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simulate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def cv_modify(X, y, Q, seed=None, oos_labels=None, unlabel_label_ratio=0.5):\n",
    "    assert Q < 50, \"Q has to be smaller than 50, try 38\"\n",
    "    assert np.all(y>0), \"unlabeled data\"\n",
    "    \n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "        \n",
    "    oos_size = 50 - Q\n",
    "    if oos_labels is None:\n",
    "        oos_labels = random.sample(range(1,51), oos_size)\n",
    "    else:\n",
    "        n = len(oos_labels)\n",
    "        assert n <= oos_size\n",
    "        assert len(set(oos_labels)) == n\n",
    "        assert all(0 < s <= 50 for s in oos_labels)\n",
    "        if n < oos_size:\n",
    "            oos_labels += random.sample(set(range(1,51)) - set(oos_labels), oos_size - n)\n",
    "\n",
    "    # for each label build a map such that the known labels are at the start followed by the unknown labels\n",
    "    label_map = [0] + sorted(set(range(1,51)) - set(oos_labels)) + sorted(oos_labels)\n",
    "    y_train  = np.array([label_map.index(yy) for yy in y])\n",
    "\n",
    "    # index of all samples that are out-of-set\n",
    "    oos = [i for i, yy in enumerate(y_train) if yy > Q or yy == 0]\n",
    "    # index of all samples that are in-set\n",
    "    in_set = [i for i, yy in enumerate(y_train) if 0 < yy <= Q]\n",
    "    \n",
    "    # take a part, r, of the samples that are in-set to be unlabeled, and leave 1-r\n",
    "    # eventually the unlabeld set will be made from Q/50 in-set samples and 1-Q/50 oos samples\n",
    "    # eventually the unlabeled size will be r*50/Q and we want\n",
    "    # (1-r)*unlabel_label_ratio = r*50/Q\n",
    "    # unlabel_label_ratio = r(50/Q + unlabel_label_ratio)\n",
    "    # r = unlabel_label_ratio/(50./Q + unlabel_label_ratio)\n",
    "    # r = Q*unlabel_label_ratio/(50. + Q*unlabel_label_ratio)\n",
    "    Qu = Q*unlabel_label_ratio\n",
    "    r = Qu/(50. + Qu)\n",
    "    in_set_unlabeled = random.sample(in_set, int(len(in_set)*r))\n",
    "    # the other half will be used as labeled\n",
    "    in_set_labeled = list(set(in_set) - set(in_set_unlabeled))\n",
    "    # give the unlabeled samples that are in-set have a high label (so the training will consider them to be unlabeled)\n",
    "    # but keep their original identity for error measurement\n",
    "    y_train[in_set_unlabeled] += 50\n",
    "\n",
    "    # add out-of-set samples to the unlabeled set keeping the ratio to labeled as before\n",
    "    oos_unlabeled = random.sample(oos,int(len(oos)*r))\n",
    "\n",
    "    unlabeled = oos_unlabeled+in_set_unlabeled\n",
    "\n",
    "    # all other (oos) samples are dropped (too bad but we want to keep the original ratios)\n",
    "    keep = in_set_labeled + unlabeled\n",
    "    random.shuffle(keep)\n",
    "\n",
    "    y_train = y_train[keep]\n",
    "    X_train = X[keep]\n",
    "    return X_train, y_train, label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q=38\n",
    "poos=0.23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_oos = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 12391 /Users/udi/Downloads/lisa/160111-fuel.train.0/160111-fuel.train.0.hdf5\n",
      "-rw-r--r--  1 udi  staff  19928000 Jan 11 18:02 /Users/udi/Downloads/lisa/160111-fuel.train.0/160111-fuel.train.0.hdf5\n",
      "12\n",
      "1 12391 /Users/udi/Downloads/lisa/160111-fuel.train.1/160111-fuel.train.1.hdf5\n",
      "-rw-r--r--  1 udi  staff  19928000 Jan 11 18:02 /Users/udi/Downloads/lisa/160111-fuel.train.1/160111-fuel.train.1.hdf5\n",
      "24\n",
      "2 12391 /Users/udi/Downloads/lisa/160111-fuel.train.2/160111-fuel.train.2.hdf5\n",
      "-rw-r--r--  1 udi  staff  19928000 Jan 11 18:02 /Users/udi/Downloads/lisa/160111-fuel.train.2/160111-fuel.train.2.hdf5\n",
      "36\n",
      "3 12391 /Users/udi/Downloads/lisa/160111-fuel.train.3/160111-fuel.train.3.hdf5\n",
      "-rw-r--r--  1 udi  staff  19928000 Jan 11 18:02 /Users/udi/Downloads/lisa/160111-fuel.train.3/160111-fuel.train.3.hdf5\n",
      "48\n",
      "4 12391 /Users/udi/Downloads/lisa/160111-fuel.train.4/160111-fuel.train.4.hdf5\n",
      "-rw-r--r--  1 udi  staff  19928000 Jan 11 18:02 /Users/udi/Downloads/lisa/160111-fuel.train.4/160111-fuel.train.4.hdf5\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "for seed in range(5):\n",
    "    # the first 5 seeds are used to cover all labels at least once\n",
    "    oos_labels = range(1+12*seed,min(1+12*seed + 12,51))\n",
    "        \n",
    "    X_train, y_train, labels = cv_modify(X[:Nt], y[:Nt], Q, seed=seed, oos_labels=oos_labels)\n",
    "    datasource = '160111-fuel.train.%d'%seed\n",
    "    datasource_dir = os.path.join(fuel.config.data_path[0], datasource)\n",
    "    datasource_fname = os.path.join(datasource_dir , datasource + '.hdf5')\n",
    "    !mkdir -p {datasource_dir}\n",
    "    N0 = len(X_train)\n",
    "    print seed, N0, datasource_fname\n",
    "\n",
    "    with h5py.File(datasource_fname, mode='w') as fp:\n",
    "        features = fp.create_dataset('features', (N0, NF), dtype=np.float32)\n",
    "        targets = fp.create_dataset('targets', (N0,), dtype='int')\n",
    "        features[...] = X_train.astype(np.float32)\n",
    "        targets[...] = y_train\n",
    "        \n",
    "        split_dict = {\n",
    "            'train': {'features': (0, N0), 'targets': (0, N0)},\n",
    "            'test': {'features': (0, N0), 'targets': (0, N0)}\n",
    "        }\n",
    "        fp.attrs['split'] = H5PYDataset.create_split_array(split_dict)\n",
    "        fp.attrs['labels'] = labels\n",
    "    !ls -l {datasource_fname}\n",
    "    oos = labels[-12:]\n",
    "    all_oos += oos\n",
    "    print len(set(all_oos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!(cd /Users/udi/Downloads/lisa/ ; tar cfz 160111-fuel.tgz 160111-fuel.train.* )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "move: ../../../../../Downloads/lisa/160111-fuel.tgz to s3://udikaggle/nist/160111-fuel.tgz\n"
     ]
    }
   ],
   "source": [
    "!aws s3 mv /Users/udi/Downloads/lisa/160111-fuel.tgz s3://udikaggle/nist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
